% This document is part of the Imaging Archetypes project.
% Copyright 2013 the Authors.

\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\newcommand{\given}{\,|\,}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\bphi}{\mbox{\boldmath $\phi$}}

\begin{document}\sloppy\sloppypar

\noindent
\begin{tabular}{ll}
\textsl{title:} & Unsupervised, hypothesis-generating robots and shit \\
\textsl{authors:} & Frean, Hogg \\
\textsl{date:}    & 2013-12-20
\end{tabular}
\bigskip

(HOGG'S TAKE ON THIS)
In the spirit of writing down what you \emph{should} do,
  and only then writing down what you \emph{actually} do,
  with connections drawn and approximations justified,
  let's dream.
The setup is that we have split our imaging up into $N$ tiny
  (possibly overlapping) patches $y_n$.
The standard kind of machine-learning thing would be to
  say that there are $K$ (possibly infinite) classes $k$,
  and that each image patch is drawn from precisely one of these classes.
Each class $k$ has a vector $\alpha_k$ of parameters
  that define a frequency distribution
  for the patches drawn from that class.
That is, there is a well defined likelihood function $p(y_n\given k, \alpha_k)$.

A model is a likelihood function plus prior pdfs.
Ideally, $p(k)$ would be an exceedingly flexible function,
  possibly even just a list of $K$ class probabilities.
Ideally $K$ itself would also have a prior probability assigned,
  although without much loss of generality
  it could just be set to a very large number up-front.
A default plan would be 
  (except possibly for one or two special classes,
   $\alpha_0$ and $\alpha_K$, for example)
  to make $p(alpha_k)$ not depend on $k$.
This is consistent with the idea that we want to work ``unsupervised''.
Putting in priors about how $\alpha_k$ should vary with $k$ is a kind of soft supervision.

That's an extremely general model,
  though it is \emph{not} as general as you can go,
  for quite a few reasons.
We are assuming we just have a ``bag of patches''.
In patchifying the imaging, we have thrown away a lot of spatial information.
We are assuming that each patch is drawn from a single class,
  never a mixture of classes.
A more general model would permit patches to be drawn from mixtures of classes;
  the likelihood function would not have a $k$ to the right of the bar,
  it would have a vector $w$ of $K$ weights.

\section{can we claim a generative model of astro images?}
(MARCUS HAS A QUESTION)

\subsection{preamble}

We (Anna and Marcus) used the $K=2$ model in which $\alpha_0$ has
large numbers in it and corresponds to the background, and $\alpha_1 =
(1,1,\ldots,1)$ and models sources. We used the ratio of the
associated posterior probs (a.k.a. the Bayes factor) for the pixel
values in a region that we parameterised as a ``score'' for the
sourciness of that region.  Optimizing the score in the space of
region parameters is source finding.

For the $i^{th}$ source, the source parameters $\btheta_i =
(x,y,w_x,w_y,\phi)$ (ie. position, widths, and rotation) could be
thought of as parameters specifying the border between source and
background if you like. We just have a strong prior we're building in
that this border tends to be elliptical.

So it's interesting to put this another way: we held the  {\bf distributions} defined by
$(\alpha_0,\alpha_1)$ {\bf fixed}, and then {\bf optimized the parameters} that
determine the sources-background boundaries.
This reminds me very much of the ``M'' step in EM. 

We didn't do the analogous E step, but could have: hold the
boundaries fixed, and move the $\alpha$ vectors.

Q: how close is the correspondence with EM here? Not even sure I've
got E and M associated right - just a hunch so far.

\subsection{generative model?}
Is our current scheme, in which we search for regions that have high
Bayes Factor values, equivalent to optimizing a parameterised
generative model for the image as a whole? We haven't thought about it
in this way before.

Writing down that model would help us to generalise.

A generative model for an image containing a single source {\it might}
go like this, assuming we're binning the intensities into $K$ bins:
\begin{enumerate}
\item make up 5 parameters $\btheta$ describing an elliptical region. For simplicity, let's assume the region has ``hard'' borders.
\item note that $\btheta$ determines the numbers of pixels inside ($N_{SRC}$)  and  outside ($N_{BG}$) the region.
\item set up $\balpha_{SRC} \in \mathbb{R}^K$, with small numbers,
  and $\balpha_{BG} \in \mathbb{R}^K$, with big numbers, especially at the bins corresponding to low amplitudes.
\item sample bin counts:
  \begin{itemize}
    \item    $\vec{n}_{SRC} \sim DM(\vec{n} \given \balpha_{SRC}, N_{SRC})$, and 
    \item $\vec{n}_{BG} \sim DM(\vec{n} \given \balpha_{BG}, N_{BG})$
  \end{itemize}
\end{enumerate}
That's it?!

\subsection{question of the day}
Our procedure is: move region parameters $\btheta$ to increase Bayes Factor $\log \frac{\Pr(\vec{n}_{SRC} \given \balpha_{SRC}, N_{SRC})}{\Pr(\vec{n}_{SRC} \given \balpha_{BG}, N_{SRC})}
{}$.

Is this the {\bf same} as finding $\btheta$ that (say) maximize the log
likelihood of the image under the above generative model???

If it's not the same, how are the two related, and which is preferable?!

\end{document}
