% This document is part of the Imaging Archetypes project.
% Copyright 2013 the Authors.

\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\newcommand{\given}{\,|\,}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\bphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bn}{\mbox{\boldmath $n$}}

\begin{document}\sloppy\sloppypar

\noindent
\begin{tabular}{ll}
\textsl{title:} & Unsupervised, hypothesis-generating robots and shit \\
\textsl{authors:} & Frean, Hogg \\
\textsl{date:}    & 2013-12-20
\end{tabular}
\bigskip

(HOGG'S TAKE ON THIS)
In the spirit of writing down what you \emph{should} do,
  and only then writing down what you \emph{actually} do,
  with connections drawn and approximations justified,
  let's dream.
The setup is that we have split our imaging up into $N$ tiny
  (possibly overlapping) patches $y_n$.
The standard kind of machine-learning thing would be to
  say that there are $K$ (possibly infinite) classes $k$,
  and that each image patch is drawn from precisely one of these classes.
Each class $k$ has a vector $\alpha_k$ of parameters
  that define a frequency distribution
  for the patches drawn from that class.
That is, there is a well defined likelihood function $p(y_n\given k, \alpha_k)$.

A model is a likelihood function plus prior pdfs.
Ideally, $p(k)$ would be an exceedingly flexible function,
  possibly even just a list of $K$ class probabilities.
Ideally $K$ itself would also have a prior probability assigned,
  although without much loss of generality
  it could just be set to a very large number up-front.
A default plan would be 
  (except possibly for one or two special classes,
   $\alpha_0$ and $\alpha_K$, for example)
  to make $p(alpha_k)$ not depend on $k$.
This is consistent with the idea that we want to work ``unsupervised''.
Putting in priors about how $\alpha_k$ should vary with $k$ is a kind of soft supervision.

That's an extremely general model,
  though it is \emph{not} as general as you can go,
  for quite a few reasons.
We are assuming we just have a ``bag of patches''.
In patchifying the imaging, we have thrown away a lot of spatial information.
We are assuming that each patch is drawn from a single class,
  never a mixture of classes.
A more general model would permit patches to be drawn from mixtures of classes;
  the likelihood function would not have a $k$ to the right of the bar,
  it would have a vector $w$ of $K$ weights.

\section{can we claim a generative model of astro images?}
(MARCUS HAS A QUESTION)

\subsection{preamble}

We (Anna and Marcus) used the $K=2$ model in which $\alpha_0$ has
large numbers in it and corresponds to the background, and $\alpha_1 =
(1,1,\ldots,1)$ and models sources. We used the ratio of the
associated posterior probs (a.k.a. the Bayes factor) for the pixel
values in a region that we parameterised as a ``score'' for the
sourciness of that region.  Optimizing the score in the space of
region parameters is source finding.

For the $i^{th}$ source, the source parameters $\btheta_i =
(x,y,w_x,w_y,\phi)$ (ie. position, widths, and rotation) could be
thought of as parameters specifying the border between source and
background if you like. We just have a strong prior we're building in
that this border tends to be elliptical.

So it's interesting to put this another way: we held the  {\bf distributions} defined by
$(\alpha_0,\alpha_1)$ {\bf fixed}, and then {\bf optimized the parameters} that
determine the sources-background boundaries.
This reminds me very much of the ``M'' step in EM. 

We didn't do the analogous E step, but could have: hold the
boundaries fixed, and move the $\alpha$ vectors.

Q: how close is the correspondence with EM here? Not even sure I've
got E and M associated right - just a hunch so far.

\subsection{generative model?}
Is our current scheme, in which we search for regions that have high
Bayes Factor values, equivalent to optimizing a parameterised
generative model for the image as a whole? We haven't thought about it
in this way before.

Writing down that model would help us to generalise.

A generative model for an image containing a single source {\it might}
go like this, assuming we're binning the intensities into $K$ bins:
\begin{enumerate}
\item make up 5 parameters $\btheta$ describing an elliptical region. For simplicity, let's assume the region has ``hard'' borders.
\item note that $\btheta$ determines the numbers of pixels inside ($N_{SRC}$)  and  outside ($N_{BG}$) the region.
\item set up $\balpha_{SRC} \in \mathbb{R}^K$, with small numbers,
  and $\balpha_{BG} \in \mathbb{R}^K$, with big numbers, especially at the bins corresponding to low amplitudes.
\item sample bin counts:
  \begin{itemize}
    \item    $\vec{n}_{SRC} \sim DCM(\vec{n} \given \balpha_{SRC}, N_{SRC})$, and 
    \item $\vec{n}_{BG} \sim DCM(\vec{n} \given \balpha_{BG}, N_{BG})$
  \end{itemize}
\end{enumerate}
where following Wikipedia ``DCM'' stands for the Dirichlet compound multinomial distribution:

\begin{align}
\text{DCM}(\bn|\alpha) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(\bn_k+\alpha_k)}{\Gamma(\alpha_k)}  
\label{eq:muldir} 
\intertext{where}
A &= \sum_k \alpha_k, \;\;\;\;\;\;\;\; N = \sum_k \bn_k
\end{align}

Taking logs,
\begin{align}
\log P(\bn|\alpha) &= \log \Gamma(A) - \log \Gamma(N+A) + \sum_k \log \Gamma(\bn_k+\alpha_k) - \log \Gamma(\alpha_k) \label{eq:logmultdir}
\end{align}


That's it?!

\subsection{question of the day}
Our procedure is: move region parameters $\btheta$ to increase Bayes Factor $\log \frac{\Pr(\vec{n}_{SRC} \given \balpha_{SRC}, N_{SRC})}{\Pr(\vec{n}_{SRC} \given \balpha_{BG}, N_{SRC})}
{}$.

Is this the {\bf same} as finding $\btheta$ that (say) maximize the log
likelihood of the image under the above generative model?
If it's not the same, how are the two related, and which is preferable?


From here I'm switching to using a superscript ``0'' for background
and ``1'' to denote source, hoping that this won't be confused with
exponentiation!

My model of the image takes parameters $\theta, \balpha^0, \balpha^1$,
and generates just one ``data point'' consisting of two vectors of
counts $\vec{n}^0,\vec{n}^1$. 


The likelihood is therefore
\begin{align}
L =& \text{DCM}(\vec{n}_0 \mid \balpha_0) \; \text{DCM}(\vec{n}_1 \mid \balpha_1) 
\end{align}

Taking logs,
\begin{align*}
\log L =& \text{logDCM}(\vec{n}_0 \mid \balpha_0) \;\; + \;\; \text{logDCM}(\vec{n}_1 \mid \balpha_1) \\ \\
% =& \log \Gamma(A^0) - \log \Gamma(N^0+A^0) + \sum_k \log \Gamma(\bn^0_k + \alpha_k^0) - \log \Gamma(\alpha_k^0) \;\; + \;\; \log \Gamma(A^1) - \log \Gamma(N^1+A^1) + \sum_k \log \Gamma(\bn^1_k + \alpha_k^1) - \log \Gamma(\alpha_k^1)  \\
 =& \log \Gamma(A^0) +\log \Gamma(A^1) - \log \Gamma(N^0+A^0)  - \log \Gamma(N^1+A^1) + \\ & \sum_k \log \Gamma(\bn^0_k + \alpha_k^0) - \log \Gamma(\alpha_k^0) + \sum_k \log \Gamma(\bn^1_k + \alpha_k^1) - \log \Gamma(\alpha_k^1) 
\end{align*}

Denote the derivative of the gamma function $\Gamma(n)$ by $\psi(n)$.

The gradient of the log likelihood w.r.t. $\theta$ is
\begin{align}
\frac{\partial}{\partial \theta} 
\log L &= 
\end{align}




\end{document}
