% This document is part of the Imaging Archetypes project.
% Copyright 2013 the Authors.

\documentclass[12pt]{article}
\newcommand{\given}{\,|\,}
\begin{document}\sloppy\sloppypar

\noindent
\begin{tabular}{ll}
\textsl{title:} & Unsupervised, hypothesis-generating robots and shit \\
\textsl{authors:} & Frean, Hogg \\
\textsl{date:}    & 2013-12-20
\end{tabular}
\bigskip

In the spirit of writing down what you \emph{should} do,
  and only then writing down what you \emph{actually} do,
  with connections drawn and approximations justified,
  let's dream.
The setup is that we have split our imaging up into $N$ tiny
  (possibly overlapping) patches $y_n$.
The standard kind of machine-learning thing would be to
  say that there are $K$ (possibly infinite) classes $k$,
  and that each image patch is drawn from precisely one of these classes.
Each class $k$ has a vector $\alpha_k$ of parameters
  that define a frequency distribution
  for the patches drawn from that class.
That is, there is a well defined likelihood function $p(y_n\given k, \alpha_k)$.

A model is a likelihood function plus prior pdfs.
Ideally, $p(k)$ would be an exceedingly flexible function,
  possibly even just a list of $K$ class probabilities.
Ideally $K$ itself would also have a prior probability assigned,
  although without much loss of generality
  it could just be set to a very large number up-front.
A default plan would be 
  (except possibly for one or two special classes,
   $\alpha_0$ and $\alpha_K$, for example)
  to make $p(alpha_k)$ not depend on $k$.
This is consistent with the idea that we want to work ``unsupervised''.
Putting in priors about how $\alpha_k$ should vary with $k$ is a kind of soft supervision.

That's an extremely general model,
  though it is \emph{not} as general as you can go,
  for quite a few reasons.
We are assuming we just have a ``bag of patches''.
In patchifying the imaging, we have thrown away a lot of spatial information.
We are assuming that each patch is drawn from a single class,
  never a mixture of classes.
A more general model would permit patches to be drawn from mixtures of classes;
  the likelihood function would not have a $k$ to the right of the bar,
  it would have a vector $w$ of $K$ weights.

\section{sort-of EM?}
Anna and I used the $K=2$ model in which $\alpha_0$ has large numbers
in it and corresponds to the background, and $\alpha_1 =
(1,1,\ldots,1)$ and models sources. We used the ratio of the
associated posterior probs (a.k.a. the Bayes factor) for the pixel
values in a region that we parameterised as a ``score'' for the
sourciness of that region.
Optimizing the score in the space of region parameters is source finding.

For the $i^{th}$ source, the source parameters $\theta_i =
(x,y,w_x,w_y,\phi)$ (ie. position, widths, and rotation) could be
thought of as parameters specifying the border between source and
background if you like. We just have a strong prior we're building in
that this border tends to be elliptical.

So it's interesting to put this another way: we held the  {\bf distributions} defined by
$(\alpha_0,\alpha_1)$ {\bf fixed}, and then {\bf optimized the parameters} that
determine the sources-background boundaries.
This reminds me very much of the ``M'' step in EM. 

We didn't do the analogous E step, but could have: hold the
boundaries fixed, and move the $\alpha$ vectors.

Q: how close is the correspondence with EM here? Not even sure I've
got E and M associated right - just a hunch so far.

Is our current $K=2, \theta$ scheme actually a parameterised
generative model for the image? I guess so. Think of a single source,
so $\theta$ a single 5-dim vector. Classic EM: E is estimate the
posterior over latent variables, given the data, which we can think of
as finding the $\alpha$'s that correspond best to the ``correct''
posterior(s) over the (latent) histograms for background and
source. Right?

Then the M step is to take those as read and optimize the parameters $\theta$.

I {\it think} this makes sense, and gives the obvious generalisation
to $K>2$ that we might make use of.

Needs some serious brainstorming...

\end{document}
