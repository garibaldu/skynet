% This document is part of the Imaging Archetypes project.
% Copyright 2013 the Authors.

\documentclass[12pt]{article}
\newcommand{\given}{\,|\,}
\begin{document}\sloppy\sloppypar

\noindent
\begin{tabular}{ll}
\textsl{title:} & Unsupervised, hypothesis-generating robots and shit \\
\textsl{authors:} & Frean, Hogg \\
\textsl{date:}    & 2013-12-20
\end{tabular}
\bigskip

In the spirit of writing down what you \emph{should} do,
  and only then writing down what you \emph{actually} do,
  with connections drawn and approximations justified,
  let's dream.
The setup is that we have split our imaging up into $N$ tiny
  (possibly overlapping) patches $y_n$.
The standard kind of machine-learning thing would be to
  say that there are $K$ (possibly infinite) classes $k$,
  and that each image patch is drawn from precisely one of these classes.
Each class $k$ has a vector $\alpha_k$ of parameters
  that define a frequency distribution
  for the patches drawn from that class.
That is, there is a well defined likelihood function $p(y_n\given k, \alpha_k)$.

A model is a likelihood function plus prior pdfs.
Ideally, $p(k)$ would be an exceedingly flexible function,
  possibly even just a list of $K$ class probabilities.
Ideally $K$ itself would also have a prior probability assigned,
  although without much loss of generality
  it could just be set to a very large number up-front.
A default plan would be 
  (except possibly for one or two special classes,
   $\alpha_0$ and $\alpha_K$, for example)
  to make $p(alpha_k)$ not depend on $k$.
This is consistent with the idea that we want to work ``unsupervised''.
Putting in priors about how $\alpha_k$ should vary with $k$ is a kind of soft supervision.

That's an extremely general model,
  though it is \emph{not} as general as you can go,
  for quite a few reasons.
We are assuming we just have a ``bag of patches''.
In patchifying the imaging, we have thrown away a lot of spatial information.
We are assuming that each patch is drawn from a single class,
  never a mixture of classes.
A more general model would permit patches to be drawn from mixtures of classes;
  the likelihood function would not have a $k$ to the right of the bar,
  it would have a vector $w$ of $K$ weights.

\end{document}
